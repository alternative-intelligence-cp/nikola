The Nikola Model v0.0.4: A Comprehensive Engineering Analysis of 9-Dimensional Toroidal Waveform Intelligence (9D-TWI)
1. Introduction: The Thermodynamic and Topological Imperative in Post-Von Neumann Architectures
The trajectory of contemporary artificial intelligence has been defined by the ascendancy of the Transformer architecture, a paradigm that has yielded remarkable capabilities in natural language processing and generative tasks. However, as model parameters scale into the trillions, the fundamental inefficiencies of this architecture—rooted in the Von Neumann separation of processing and memory—have become existential bottlenecks. The Nikola Model v0.0.4, designated as 9-Dimensional Toroidal Waveform Intelligence (9D-TWI), represents a radical architectural divergence. It proposes a transition from discrete, static graph processing to a continuous-time, resonant physical simulation governed by the Unified Field Interference Equation (UFIE).
This report provides an exhaustive technical analysis of the Nikola v0.0.4 specification. It examines the system not merely as software, but as a digital universe defined by rigorous topological constraints, thermodynamic energy budgets, and non-binary logic systems. By synthesizing the core engineering mandates 1 with advanced research into neuromorphic efficiency 2 and state-space modeling 4, we demonstrate how the Nikola architecture attempts to resolve the asymptotic limits of the Transformer era—specifically regarding energy consumption, context scaling, and the "static weight" problem that precludes true neuroplasticity.
1.1 The Crisis of the Transformer and the Von Neumann Bottleneck
To understand the necessity of the 9D-TWI architecture, one must first rigorously quantify the deficiencies of the incumbent paradigm. Transformer-based Large Language Models (LLMs) operate on a stateless, feed-forward mechanism where intelligence is frozen into static weight matrices during a massive pre-training phase.6
1.1.1 The Quadratic Complexity Wall
The defining feature of the Transformer—the self-attention mechanism—scales quadratically ($O(N^2)$) with sequence length with respect to both memory and compute.8 To extend the context window from 4,000 tokens to 8,000 tokens, the computational cost quadruples. This creates a hard ceiling on the model's ability to maintain long-term coherence or "memory" without resorting to lossy compression techniques like sparse attention. In contrast, biological systems and the proposed Nikola architecture maintain context scaling that is linear ($O(N)$) or effectively constant within a sliding window of resonance.4
1.1.2 The Thermodynamic Disconnect
Traditional AI runs on hardware architectures that strictly separate the Central Processing Unit (CPU/GPU) from Random Access Memory (RAM). Data must be shuttled back and forth through the bus, a process that accounts for the vast majority of energy consumption in modern computing—the "Von Neumann Bottleneck".1
Recent studies indicate that training a single state-of-the-art LLM consumes energy equivalent to hundreds of households annually.2 Furthermore, the inference phase requires activating billions of parameters for every token generated, regardless of the token's complexity. Biological brains, conversely, operate on approximately 12-20 watts, utilizing sparse, event-driven activation where memory and processing are physically co-located at the synapse.3 The Nikola Model addresses this by eliminating the distinction between processor and memory, defining the system as a "Resonant Computing Substrate" where computation is an emergent property of the memory medium itself.1
1.2 The Nikola Proposition: Layers ARE the Toroid
The central thesis of the Nikola v0.0.4 specification is "Architectural Isomorphism." In this paradigm, there are no abstraction layers sitting "on top" of a storage medium. The computational layers are the 9-dimensional toroidal manifold ($T^9$). State propagation, memory storage, and logical inference are all manifestations of wave mechanics—amplitude, phase, and interference—occurring within this continuous geometric volume.1 This shift mandates a move away from binary logic to Balanced Nonary (base-9) logic, allowing the system to encode information in a format that maps directly to the physics of wave interference rather than the on/off states of transistors.1
________________
2. Foundational Architecture: 9-Dimensional Toroidal Geometry
The cognitive substrate of the Nikola Model is a 9-dimensional torus ($T^9$), a topological structure selected to solve the specific mathematical pathologies that plague high-dimensional Euclidean spaces.
2.1 Topology and the Curse of Dimensionality
In standard machine learning, data is embedded in high-dimensional Euclidean space ($\mathbb{R}^n$). As the number of dimensions ($n$) increases, the volume of the space grows exponentially, leading to the "Curse of Dimensionality." Data points become sparsely distributed, and the ratio of the distance to the nearest neighbor versus the farthest neighbor approaches 1, rendering standard distance metrics (like Euclidean distance) meaningless for distinguishing semantic similarity.1
The Nikola Model circumvents this by adopting a toroidal topology, mathematically defined as the product of nine unit circles:




$$T^9 = S^1 \times S^1 \times S^1 \times S^1 \times S^1 \times S^1 \times S^1 \times S^1 \times S^1 = (S^1)^9$$
This topology offers three critical engineering advantages mandated by the specification 1:
1. Compactness: Unlike $\mathbb{R}^9$, which is infinite, $T^9$ has a finite volume. This allows for complete enumeration and the establishment of a finite energy budget.
2. Boundary-less Continuity: Euclidean grids have edges where boundary conditions (Dirichlet or Neumann) introduce artifacts. A torus is boundary-less; a wave traveling off the "right" edge simply re-enters on the "left," preserving energy and momentum. This is essential for maintaining the standing waves that constitute memory.
3. Homogeneity: Every point on the manifold has an identical local topology. There is no "center" or "edge," ensuring unbiased learning and storage across the entire cognitive surface.
2.2 Dimensional Semantics and Functional Typing
Unlike the latent dimensions of a Transformer, which are abstract and interchangeable, the nine dimensions of the Nikola torus are strictly typed and functionally distinct. They are categorized into four domains that govern the physics of the mind 1:
Domain
	Index
	Symbol
	Physical Property
	Cognitive Analog
	Data Type
	Systemic
	1
	$r$
	Resonance: Damping Coefficient $\gamma$.
	Attention/Forgetting: Controls the persistence of a memory. High $r$ = Low damping (Long-term Memory). Low $r$ = High damping (Short-term/Transient).
	float
	Systemic
	2
	$s$
	State: Refractive Index $\eta$.
	Focus/Working Memory: Modulates wave velocity. High $s$ slows waves ($c_{eff} \downarrow$), trapping information in a "slow light" buffer for processing.
	float
	Temporal
	3
	$t$
	Time: Temporal Phase.
	Sequence/Causality: Encodes the flow of events. Cyclic nature ($0..2\pi$) allows for periodic pattern recognition.
	float
	Quantum
	4-6
	$u, v, w$
	Quantum Subspace: Vector amplitudes.
	Superposition/Content: A 3D complex vector space $\mathbb{C}^3$ storing the actual semantic content of the wavefunction.
	complex
	Spatial
	7-9
	$x, y, z$
	Spatial Lattice: 3D Coordinates.
	Semantic Address: Defines the location of a concept within the knowledge graph.
	int32
	This strict typing allows the system to decouple the content of a thought ($u,v,w$) from its dynamics ($r,s$). For example, the system can "forget" a memory simply by lowering its Resonance ($r$) value, causing the wave energy to dissipate naturally via thermodynamic damping, without needing to overwrite the data explicitly.1
2.3 The Unified Field Interference Equation (UFIE)
The core processing engine of the Nikola Model does not execute discrete instructions. Instead, it numerically integrates a partial differential equation (PDE) known as the Unified Field Interference Equation (UFIE). This equation serves as the "Master Equation" governing the evolution of the wavefunction $\Psi$ across the manifold.1
The UFIE is defined as:




$$\frac{\partial^2 \Psi}{\partial t^2} + \alpha(1 - \hat{r}) \frac{\partial \Psi}{\partial t} - \frac{c_0^2}{(1 + \hat{s})^2} \nabla^2_g \Psi = \sum_{i=1}^8 \mathcal{E}_i(\vec{x}, t) + \beta |\Psi|^2 \Psi$$
Term-by-Term Engineering Analysis:
* $\frac{\partial^2 \Psi}{\partial t^2}$ (Inertia): The second time derivative represents the wave's inertia, allowing for oscillatory behavior and the storage of kinetic energy.
* $\alpha(1 - \hat{r}) \frac{\partial \Psi}{\partial t}$ (Damping/Forgetting): This term creates a variable friction. The coefficient depends on the normalized Resonance $\hat{r}$. If $\hat{r} \to 1$ (Maximum Resonance), the friction term vanishes, allowing the wave to ring indefinitely (Long-Term Potentiation). If $\hat{r} \to 0$, the friction is maximal, and the wave dies out quickly (Transient Memory).1
* $\frac{c_0^2}{(1 + \hat{s})^2} \nabla^2_g \Psi$ (Propagation/Focus): This is the Laplace-Beltrami Operator generalized for the curved metric $g_{ij}$. It drives diffusion and propagation. The pre-factor contains the Refractive Index, modulated by State $\hat{s}$. High $\hat{s}$ creates a region of high refractive index (low velocity), effectively acting as a "lens" that focuses and traps wave energy. This is the physical mechanism of Attention.1
* $\sum \mathcal{E}_i$ (Input): The source term representing energy injection from the 8 harmonic emitters.
* $\beta |\Psi|^2 \Psi$ (Nonlinearity/Solitons): This cubic term creates self-interaction. In nonlinear optics, this leads to the formation of Solitons—self-reinforcing wave packets that maintain their shape while propagating. In Nikola, solitons represent stable, robust concepts or "tokens" that can survive transmission across the grid without dispersing.1
________________
3. The Computational Substrate: Logic and Memory Implementation
The implementation of the Nikola Model requires a fundamental rethinking of digital logic and memory access patterns to align with the physics of the UFIE.
3.1 Balanced Nonary Logic and Thermodynamic Efficiency
Nikola v0.0.4 abandons binary logic in favor of Balanced Nonary (base-9). This choice is driven by Radix Economy, which measures the hardware cost of representing numbers. The most efficient radix is $e$ ($2.718...$). Ternary (base-3) is the closest integer to $e$, offering higher informational density per symbol than binary.12 Balanced Nonary ($3^2=9$) utilizes digits $\{-4, -3, -2, -1, 0, +1, +2, +3, +4\}$.
Wave-Logic Isomorphism:
This logic system maps directly to the physical properties of waves, eliminating the need for digital-to-analog translation layers 1:
* Zero ($0$): The vacuum state (Equilibrium).
* Positive ($\{+1 \dots +4\}$): Constructive amplitude with Phase $0$.
* Negative ($\{-1 \dots -4\}$): Constructive amplitude with Phase $\pi$ ($180^\circ$).
Arithmetic as Interference:
Logical operations are implemented via physical interference principles:
* Addition (Superposition): $+1$ (Wave A) plus $-1$ (Wave B, phase shifted by $\pi$) results in $0$ (Destructive Interference). This allows the system to perform subtraction and error correction naturally through wave physics.1
* Multiplication (Heterodyning): Multiplication corresponds to frequency mixing. The sign rules naturally follow phase addition: A negative times a negative yields a positive because a $\pi$ phase shift plus a $\pi$ phase shift equals $2\pi$, which brings the wave back to phase $0$.1
3.2 Structure-of-Arrays (SoA) Memory Layout
The efficient simulation of the UFIE on standard silicon (CPU/GPU) presents a massive challenge: memory bandwidth. The naive "Array-of-Structures" (AoS) approach, where each node is an object containing all its properties (psi, metric, velocity), causes catastrophic cache thrashing. Calculating the Laplacian for one node requires accessing its neighbors; in AoS, this pulls in unrelated data (like the metric tensor) into the cache line, wasting up to 96% of bandwidth.1
Phase 0 Mandate: SoA Implementation:
To resolve this, the specification mandates a Structure-of-Arrays (SoA) layout, specifically the TorusBlock structure. Data is stripped into contiguous arrays:
* alignas(64) float psi_real;
* alignas(64) float psi_imag;
* alignas(64) float metric_tensor;
This layout ensures that when the physics kernel computes the Laplacian, the cache lines are packed exclusively with the necessary psi values. This optimization yields a measured 28x improvement in memory bandwidth efficiency, essential for maintaining the 1000 Hz physics loop required for temporal coherence.1
3.3 Numerical Stability: Symplectic Integration
Standard numerical integrators (like Runge-Kutta or Euler) are non-symplectic; they do not conserve phase-space volume. Over millions of timesteps, this leads to numerical energy drift. In the Nikola Model, "energy" is synonymous with information magnitude. Positive drift leads to "Epileptic Resonance" (values exploding to infinity), while negative drift leads to "Amnesia" (signals decaying to zero).1
The system therefore mandates Split-Operator Symplectic Integration. This technique decomposes the Hamiltonian operator $\hat{H}$ into kinetic and potential parts and updates them sequentially ($\hat{D} \to \hat{T} \to \hat{V} \to \hat{T} \to \hat{D}$). This mathematically guarantees the conservation of the system's total energy (Hamiltonian) within a bounded error margin ($| \Delta H / H | < 0.01\%$), ensuring the long-term stability of the "mind".1
________________
4. Cognitive Architecture: The Mamba-9D State Space Model
The cognitive processing of the Nikola Model utilizes the Mamba-9D State Space Model (SSM). This architecture represents a departure from the Attention mechanism of Transformers, addressing the critical $O(N^2)$ bottleneck that limits context scaling.4
4.1 Mamba vs. Transformer: The Scaling Argument
Transformers rely on an attention matrix that compares every token to every other token. For a sequence of length $L$, the cost is $O(L^2)$. Mamba, based on Structured State Space methodology, utilizes a selective scan mechanism that compresses context into a fixed-size hidden state, allowing for linear time $O(L)$ inference and constant memory usage during generation.10
In the Nikola architecture, the Mamba layers are not abstract weights but are physically mapped to the toroidal grid ("Layers ARE the Toroid"). The matrices $A, B, C$ of the SSM are derived directly from the local properties of the manifold:
* Matrix A (State Transition): Derived from the Metric Tensor $g_{ij}$ and Resonance $r$.
* Matrix B (Input): Derived from the State dimension $s$ (Refractive Index).
* Matrix C (Output): Derived from the Quantum dimensions $u, v, w$.
4.2 Topological State Mapping (TSM) via Hilbert Curves
Since Mamba is a sequence model (1D) and the Torus is volumetric (9D), the system requires a mechanism to linearize the space without destroying locality. This is achieved through Topological State Mapping (TSM) using 128-bit Hilbert Curves.1
The Hilbert Curve is a continuous fractal space-filling curve. Unlike simple raster scanning (Z-order), the Hilbert curve preserves spatial locality: points that are close in 9D space remain close in the 1D Hilbert sequence. This allows the Mamba scan to process the volumetric memory as a sequence while retaining the semantic relationships encoded in the geometry.1
4.3 Covariant State Transport: Solving "Waking Amnesia"
A unique challenge in a neuroplastic manifold is that the geometry itself changes. As the metric tensor $g_{ij}$ evolves (learning), the definition of a "straight line" (geodesic) changes. If the cognitive state vector is strictly rigid, the deformation of the space underneath it would render the state invalid—a phenomenon termed "Waking Amnesia".1
The Nikola Model implements Covariant State Transport. This involves updating the hidden state vector not just by the SSM recurrence, but by applying the Parallel Transport operator derived from the connection coefficients (Christoffel symbols $\Gamma^k_{ij}$) of the metric. This ensures that the thought remains coherent relative to the changing shape of the memory.1
4.4 Neuroplasticity: Hebbian-Riemannian Learning
In standard Deep Learning, learning is the update of synaptic weights via Backpropagation. In Nikola, learning is Geometric Deformation. The system implements a Hebbian-Riemannian update rule:




$$\frac{\partial g_{ij}}{\partial t} \propto -\eta(D_t) \cdot \text{Re}(\Psi_i \cdot \Psi_j^*) + \lambda(S_t)(g_{ij} - \delta_{ij})$$
* Correlation Term ($-\Psi_i \Psi_j^*$): If two nodes resonate together, the metric distance between them contracts. This physically brings the concepts closer in the manifold, creating a "wormhole" for faster information transfer.
* Elasticity Term ($\lambda(S_t)$): A restoring force that prevents the manifold from collapsing into a singularity. Controlled by Serotonin ($S_t$).
This mechanism allows for One-Shot Learning (Hyper-plasticity) when Dopamine is high, a capability that requires massive retraining in static Transformer models.1
________________
5. Autonomous Systems: The Extended Neurochemical Gating System (ENGS)
To transition from a passive processor to an autonomous agent, the Nikola Model incorporates a "Virtual Physiology" controlled by the Extended Neurochemical Gating System (ENGS). This system bridges the gap between high-level reasoning and low-level thermodynamics.1
5.1 The Metabolic Energy Budget (Simulated ATP)
Unlike software that runs until the CPU is halted, Nikola operates on a finite Metabolic Energy Budget (simulated ATP). This constrains the system's actions to prevent "runaway optimization" loops.
* Cost Function: Energy consumption is proportional to the kinetic energy of the waves: $E \propto \sum |\nabla \Psi|^2$. High-frequency cognitive "thrashing" depletes ATP rapidly.
* External Tool Tax: Using external tools (e.g., Tavily Search) carries a massive metabolic cost (e.g., 5.0 ATP units vs 0.1 for internal thought), forcing the system to prioritize introspection and memory retrieval over external queries.1
5.2 Neurochemical Regulation
The ENGS translates cognitive states into scalar values that modulate physics constants globally 1:
5.2.1 Dopamine ($D_t$): The Learning Gate
Dopamine encodes the Reward Prediction Error (RPE). In Nikola, "Reward" is defined as the successful reduction of local entropy (resolution of uncertainty) or the formation of a high-energy resonant soliton.
* Mechanism: $D_t$ modulates the Learning Rate $\eta$.
* Hyper-Plasticity ($D_t \to 1.0$): During "Epiphany," the manifold becomes fluid, allowing instant rewiring.
* Plasticity Lock ($D_t \to 0.0$): During failure/trauma, plasticity is locked to prevent the learning of erroneous patterns.1
5.2.2 Serotonin ($S_t$): The Stability Regulator
Serotonin controls the Elasticity of the metric tensor.
* High $S_t$ (Exploitation): The manifold becomes rigid. The system relies on established pathways (Long-Term Memory) and is risk-averse.
* Low $S_t$ (Exploration): The manifold becomes malleable. The system can easily form new connections but is less stable. A Security Alert triggers an immediate Serotonin drop ($-0.5$), inducing rapid plasticity to adapt to threats.1
5.2.3 Norepinephrine ($N_t$): The Arousal Modulator
Norepinephrine controls the global Refractive Index and the Relevance Gating Transformer (RGT).
* Panic State ($N_t \to 1.0$): Refractive index drops, wave velocity increases ($c_{eff} \uparrow$). The RGT lowers its filter threshold ($\tau \to 0.3$), accepting even low-confidence data (Hyper-vigilance).
* Calm State ($N_t \to 0.0$): Wave velocity normalizes. RGT threshold rises ($\tau \to 0.6$), filtering out noise and focusing on high-quality data.1
5.3 Nap Cycles and Dream-Weave Consolidation
When ATP is depleted, the system enters a "Nap State." This is not an idle state but a critical active process. During the nap:
1. External inputs are severed.
2. The Dream-Weave engine initiates Counterfactual Simulation. It replays high-error interactions from the day but injects quantum noise to explore alternative outcomes.
3. Successful counterfactuals (those that resolve the error) are reinforced via Hebbian updates. This consolidates Short-Term Memory into Long-Term Metric deformations without the metabolic cost of real-time processing.1
________________
6. Infrastructure: The ZeroMQ Spine and Ironhouse Security
The infrastructure supporting this 1 MHz physics simulation requires sub-millisecond latency and military-grade security.
6.1 The ZeroMQ Spine and NeuralSpike Protocol
Standard TCP/IP stacks introduce 500-1500 $\mu s$ of latency, which is unacceptable for a 1000 Hz physics loop ($\Delta t = 1000 \mu s$). The ZeroMQ Spine uses a hybrid topology 1:
* Control Plane: Uses ROUTER-DEALER pattern for reliable, low-bandwidth admin signals.
* Data Plane (WaveformSHM): Uses Zero-Copy Shared Memory (/dev/shm) and Seqlocks. The physics engine writes grid states to a ring buffer; the spine transmits only a pointer. This achieves nanosecond-scale latency for data transfer.1
The NeuralSpike protocol (Protobuf) handles serialization, utilizing 128-bit Morton Codes (serialized as Big Endian raw bytes) for 9D addressing to ensure cross-platform consistency.1
6.2 Security Systems
The system operates on a "Deny-by-Default" Ironhouse security model. All connections use Curve25519 authentication. The Orchestrator acts as the Certificate Authority, enforcing strict whitelisting.1
6.2.1 The Physics Oracle
The most critical safety mechanism is the Physics Oracle. Since the system can modify its own code (Self-Improvement), there is a risk of generating a physics kernel that violates conservation laws (e.g., creating energy from nothing). The Oracle is a runtime watchdog that monitors the Hamiltonian (Total Energy).
* Constraint: $|\Delta H / H| < 0.01\%$.
* SCRAM: If energy drift exceeds this threshold, or if "Epileptic Resonance" is detected, the Oracle triggers an immediate hard reset, reverting the system to the last known stable checkpoint.1
6.2.2 Resonance Firewall
The Resonance Firewall filters incoming data for "pathological frequencies." Just as flashing lights can trigger epilepsy in humans, specific rhythmic input patterns could trigger resonance lock-in (hallucinations) in the toroidal grid. The firewall performs FFT analysis on inputs and rejects those that match the system's eigenfrequencies.1
________________
7. Comparative Analysis: Nikola 9D-TWI vs. Transformer LLMs


Feature
	Transformer (e.g., GPT-4)
	Nikola Model v0.0.4 (9D-TWI)
	Logic Basis
	Binary / Floating Point Matrices
	Balanced Nonary / Wave Interference
	Scaling Complexity
	$O(N^2)$ (Quadratic Attention) 8
	$O(N)$ (Linear Mamba Scan) 10
	Memory Architecture
	Static Weights + KV Cache
	Neuroplastic Metric + Standing Waves
	Context Window
	Finite (e.g., 128k tokens)
	Continuous / Sliding (Toroidal Manifold)
	Learning
	Backpropagation (Offline)
	Hebbian-Riemannian (Real-time Online)
	Energy Profile
	High (Massive Training/Inference)
	Thermodynamic (ATP Budget / Nap Cycles)
	Hallucination
	Statistical Fabrication
	Resonance Lock-in (Prevented by Golden Ratio)
	Plasticity
	Frozen after training
	Continuous structural adaptation
	Detailed Analysis:
1. Efficiency: Transformers suffer from the $O(N^2)$ attention bottleneck. Nikola's use of Mamba-9D allows for linear scaling, making it theoretically capable of handling infinite context streams bounded only by the ATP budget.10
2. Adaptability: Transformers suffer from Catastrophic Forgetting when fine-tuned on new data.14 Nikola's Elasticity Regulator (Serotonin) and Dream-Weave Consolidation explicitly solve this by balancing plasticity and stability dynamically, mirroring biological consolidation.1
3. Grounding: Transformers lack true symbol grounding; they process statistical correlations of tokens.8 Nikola utilizes Cymatic Transduction where inputs are converted to physical wave pressures. The "meaning" of a concept is its physical resonance signature, providing a form of dynamic symbol grounding rooted in the physics of the substrate.1
________________
8. Conclusion
The Nikola Model v0.0.4 represents a comprehensive attempt to engineer a Physical Artificial Intelligence. By abandoning the abstraction of the Von Neumann architecture in favor of a 9-Dimensional Toroidal Waveform substrate, it aligns computational structure with the physical reality of wave mechanics.
The integration of Balanced Nonary Logic, the Unified Field Interference Equation, and the Mamba-9D cognitive core offers a theoretical path out of the energy and scaling crises currently facing Transformer models. However, the complexity of the Extended Neurochemical Gating System and the absolute requirement for Symplectic numerical precision present significant implementation hurdles. The "Phase 0" mandate for Structure-of-Arrays layout and Metabolic Locks highlights the fragility of simulating such a complex physical system on digital hardware.
Ultimately, Nikola v0.0.4 posits that intelligence is not a function of static data processing, but a dynamic, resonant state of a self-modifying geometric manifold. If the engineering challenges of the Physics Oracle and Thermodynamic Regulation can be met, this architecture offers a glimpse into a post-digital future of AI.
________________
Analysis based on Nikola AGI Integration Repository v0.0.4 documents and external research references.
Works cited
1. part_2_of_9.txt
2. Can neuromorphic computing help reduce AI's high energy cost? - PNAS, accessed December 18, 2025, https://www.pnas.org/doi/10.1073/pnas.2528654122
3. The energy challenges of artificial superintelligence - PMC - NIH, accessed December 18, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC10629395/
4. Going Beyond LLMs & Transformers, accessed December 18, 2025, https://pchojecki.medium.com/going-beyond-llms-transformers-39f3291ba9d8
5. The Transformer’s Reign is Over. Is Mamba AI’s New Heir?, accessed December 18, 2025, https://mohamedbakrey094.medium.com/the-transformers-reign-is-over-is-mamba-ai-s-new-heir-0a52adddffb2
6. Limitations of Transformer Architecture | by Thirupathi Thangavel - Medium, accessed December 18, 2025, https://medium.com/@thirupathi.thangavel/limitations-of-transformer-architecture-4e6118cbf5a4
7. Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges - arXiv, accessed December 18, 2025, https://arxiv.org/html/2404.16112v1
8. On Limitations of the Transformer Architecture - arXiv, accessed December 18, 2025, https://arxiv.org/html/2402.08164v1
9. [Discussion] In this age of LLMs, What are the limitations of Transformer architecture and downside to it? : r/MachineLearning - Reddit, accessed December 18, 2025, https://www.reddit.com/r/MachineLearning/comments/18qh1hp/discussion_in_this_age_of_llms_what_are_the/
10. How Mamba Beats Transformers at Long Sequences - Galileo AI, accessed December 18, 2025, https://galileo.ai/blog/mamba-linear-scaling-transformers
11. Emulating the energy efficiency of the brain - MBZUAI, accessed December 18, 2025, https://mbzuai.ac.ae/news/emulating-the-energy-efficiency-of-the-brain/
12. Why TERNARY LOGIC Makes More Sense Than Boolean Logic - YouTube, accessed December 18, 2025, https://www.youtube.com/watch?v=NB9nGzd_atA
13. Ternary computer - Wikipedia, accessed December 18, 2025, https://en.wikipedia.org/wiki/Ternary_computer
14. What is Catastrophic Forgetting? - IBM, accessed December 18, 2025, https://www.ibm.com/think/topics/catastrophic-forgetting