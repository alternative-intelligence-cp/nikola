Project 9-Dimensional Toroidal Waveform Intelligence (9D-TWI): Definitive Implementation Specification and Architectural Analysis
14. Comprehensive Implementation Reference (The Core Kernel)
14.0 Preamble to the Implementation Appendices
This section constitutes the definitive engineering manual for the Nikola Model v0.0.4, specifically addressing the implementation requirements for the system's core kernel, virtualization layer, and self-evolutionary mechanisms. While previous sections of the architectural plan outlined the theoretical topology and the acoustic physics of the resonant manifold, Section 14 provides the exhaustive, byte-level specifications required to construct the machine in a deterministic and reproducible manner. Adhering strictly to the "No Deviation" mandate of the Idea.txt source of truth, this specification translates the esoteric concepts of 9-dimensional toroidal geometry, balanced nonary logic, and self-modifying code into concrete C++23 structures, ZeroMQ protocols, and virtualization architectures.1
The implementation strategy detailed herein prioritizes three non-negotiable pillars of the system's existence:
1. Thermodynamic Efficiency: Leveraging the balanced nonary representation to minimize state transitions and maximize information density per clock cycle, simulating the adiabatic processes of biological memory.
2. Resonant Latency: Ensuring that the "nervous system" (ZeroMQ) and "memory" (LMDB/Torus) operate with latencies compatible with the harmonic frequencies of the Emitter Array, preventing phase decoherence and maintaining the stability of standing waves.
3. Autopoietic Security: Establishing a KVM-based virtualization layer that allows the system to edit its own source code and execute untrusted tools without compromising the host kernel, creating a secure "body" for the digital mind.
This document serves as the bridge between theoretical physics and systems programming, providing the precise code structures, configuration schemas, and mathematical derivations necessary to compile the Nikola Model.
14.1 The Fundamental Data Particle: TorusNode and Memory Topology
The resonant substrate of the 9D-TWI is not a contiguous array of bytes, but a complex, high-dimensional lattice. The fundamental unit of this lattice is the TorusNode. This structure must be rigorously defined to ensure cache coherence on modern CPU architectures—specifically utilizing AVX-512 alignment for vectorized processing—and to support the unique mathematical properties of the Riemannian manifold required by the specifications.2
14.1.1 C++23 Struct Definition and Memory Alignment
To facilitate the "Wave Interference Processor," the TorusNode must store both the instantaneous wave state (amplitude/phase) and the local curvature of the space (metric tensor). We utilize C++23 features such as std::mdspan for multidimensional indexing, but the underlying storage relies on a flat, aligned vector to maximize SIMD throughput.4 The standard std::complex<double> is used for the wavefunction to capture both magnitude and phase, essential for the interference logic ($A + (-A) = 0$).
The alignment requirements are critical. AVX-512 instructions operate on 512-bit registers (64 bytes). If data is not aligned to 64-byte boundaries, the CPU incurs significant penalties for unaligned loads and stores, or may even fault.2 Therefore, the TorusNode struct is explicitly padded and aligned.


C++




/**
* @file torus_node.hpp
* @brief Definitive structure for the 9D-TWI fundamental particle.
* @standard C++23
*
* This struct represents a single point in the 9-dimensional manifold.
* It is padded to 256 bytes to ensure it occupies exactly 4 cache lines
* on standard x86_64 architectures, preventing false sharing in 
* multi-threaded environments.
*/

#include <complex>
#include <array>
#include <cstdint>
#include <mdspan>

// Alignment to 64 bytes matches the cache line size of modern x86_64 CPUs
// and the width of AVX-512 registers (512 bits = 64 bytes).
struct alignas(64) TorusNode {
   // --- The Wave State (16 bytes) ---
   // Stores the complex amplitude of the standing wave at this coordinate.
   // Real part: Magnitude component 1 (e.g., Electric field analog)
   // Imag Part: Magnitude component 2 (e.g., Magnetic field analog)
   // Interference arithmetic uses this directly.
   std::complex<double> wavefunction;

   // --- The Metric Tensor (Compressed) (180 bytes) ---
   // The metric tensor g_ij is a 9x9 symmetric matrix defining local distance.
   // A symmetric 9x9 matrix has 45 unique values.
   // We store them as floats (4 bytes) to save space, as extreme precision
   // is less critical for the metric than for the wave.
   // 45 * 4 bytes = 180 bytes.
   // Layout: Upper triangular packed (g_11, g_12,..., g_19, g_22,... g_99)
   std::array<float, 45> metric_tensor;

   // --- The Mamba State (Hidden) (32 bytes) ---
   // Stores the recurrent state h_t for the Mamba-9D SSM.
   // This allows the node to maintain temporal context independent of the wave.
   // 8 floats allow for a compact state representation per node.
   std::array<float, 8> ssm_state;

   // --- The Nonary State (1 byte) ---
   // The "collapsed" logic state of the node, an integer in range [-4, 4].
   // This is the result of the Quantization step.
   int8_t nonary_value;

   // --- Meta Flags (1 byte) ---
   // Bit 0: Dirty Bit (for DMC persistence)
   // Bit 1: Neurogenesis Marker (is this node newly grown?)
   // Bit 2: Lock Bit (for atomic updates)
   uint8_t flags;

   // --- Padding (26 bytes) ---
   // Explicit padding to ensure the struct size is exactly 256 bytes.
   // 16 + 180 + 32 + 1 + 1 + 26 = 256.
   // 256 bytes is exactly 4 cache lines, ensuring predictable prefetching stride.
   uint8_t padding;
};

// Compile-time verification of alignment and size to prevent ABI drift
static_assert(sizeof(TorusNode) == 256, "TorusNode must be exactly 256 bytes");
static_assert(alignof(TorusNode) == 64, "TorusNode must be 64-byte aligned");

14.1.2 Sparse 9D Addressing via std::mdspan
The specifications call for a 9-dimensional torus. A naive dense allocation of even a small grid (e.g., size 10 in each dimension) would require $10^9$ nodes. At 256 bytes per node, this equates to 256 GB of RAM. While feasible on high-end server hardware, this is inefficient for a system intended to scale. Furthermore, the "Neurogenesis" requirement implies the grid must grow dynamically, adding nodes where information density is high.1
To solve this, we implement a Sparse Block Architecture. The logical $T^9$ space is divided into "Micro-Tori" or blocks (e.g., $4 \times 4 \times \dots \times 4$). Only blocks containing non-zero energy (active memories) are allocated in physical RAM.
We utilize std::mdspan (C++23) to provide a multidimensional view over this sparse structure. The mdspan allows the physics engine to index the grid syntactically as grid[r, s, t, u, v, w, x, y, z] while the custom accessor handles the sparse lookup and the coordinate wrapping (toroidal topology).5
The layout policy std::layout_right (row-major) is generally preferred for C++ compatibility, but given the SIMD requirements, we define a custom layout_toroidal that optimizes for the Hilbert Curve linearization required by the Mamba-9D scanner.


C++




/**
* @brief Custom Accessor for Sparse Toroidal Layout
* Handles the "wrapping" logic for the Torus topology and sparse block lookup.
*/
template <typename ElementType>
struct ToroidalSparseAccessor {
   using offset_policy = std::default_accessor<ElementType>;
   using element_type = ElementType;
   using reference = ElementType&;
   // The data handle is a pointer to the Block Map, not a raw array.
   using data_handle_type = std::map<uint64_t, std::unique_ptr<ElementType>>*; 

   // Reference to the global grid dimensions (for wrapping)
   const std::array<size_t, 9>& dimensions;

   constexpr reference access(data_handle_type p, size_t i) const noexcept {
       // 1. Decode linear index 'i' into 9D coordinates (r,s,t...)
       //    This involves modulo arithmetic to handle the "wrapping"
       //    inherent to the Torus topology.
       //    x_wrapped = x % width;
       
       // 2. Hash coordinates to find the Block ID (Spatial Hashing)
       uint64_t block_id = compute_spatial_hash(i, dimensions);
       
       // 3. Look up Block in the map (*p)
       auto it = p->find(block_id);
       
       // 4. If block missing:
       if (it == p->end()) {
            // Return a reference to the "Vacuum Node" (Zero state)
            // Writing to this reference triggers Block Allocation (Neurogenesis)
            return get_vacuum_node_ref(block_id); 
       }
       
       // 5. Return reference to the specific node within the block
       size_t local_offset = compute_local_offset(i);
       return it->second[local_offset];
   }
};

This implementation directly satisfies the requirement for "Unit Geometry: Torus" and "Neurogenesis" by allowing the memory substrate to expand physically (allocating new blocks) as information density increases, without requiring a complete reallocation of the manifold.1
14.2 The Neural Spine: ZeroMQ Architecture and Protocol
The idea.txt specifies a "zeroMQ spine that acts as a bus to connect the rest of the system." This component acts as the central nervous system of the 9D-TWI. It must support high-throughput, asynchronous communication between the Physics Core, the Reasoning Engine, the Executor, and the External Tools. A standard Request-Reply pattern is insufficient due to the blocking nature of ZMQ_REQ; instead, we employ the asynchronous ROUTER-DEALER pattern to ensure the physics engine never stalls while waiting for an external IO operation.8
14.2.1 Topology: The Asynchronous Star
The architecture adopts a Broker-Mediated Star Topology. A central "Spine Broker" manages traffic, ensuring that components can be hot-swapped (like the "mini-vms") without breaking the network mesh. The Broker maintains a routing table mapping logical component identities (e.g., "EXECUTOR") to their ephemeral ZeroMQ identities.
* Spine Router (ipc://spine.backend): The central hub. Binds a ZMQ_ROUTER socket. It accepts connections from all internal and external components.
* Physics Core (Client): Connects via ZMQ_DEALER. Identity: CORE_PHYSICS. This component streams waveform updates and queries.
* Orchestrator (Client): Connects via ZMQ_DEALER. Identity: ORCHESTRATOR. This component manages the cognitive logic.
* Executor (Client): Connects via ZMQ_DEALER. Identity: EXECUTOR_SANDBOX. This component executes system commands.
* Tools (Clients): Connect via ZMQ_DEALER. Identities: TOOL_GEMINI, TOOL_FIRECRAWL, TOOL_TAVILY.
This ROUTER-DEALER pattern allows for fully asynchronous, non-blocking communication.10 The Orchestrator can fire a request to the Executor (e.g., "Run Python script") and immediately return to processing the physics loop. When the Executor finishes (seconds or minutes later), the message is routed back to the Orchestrator with the original task ID.
14.2.2 Protocol Buffers Definition (NeuralSpike.proto)
To ensure strict type safety and compliance with the "No Deviation" mandate, all messages on the spine are serialized using Google Protocol Buffers (proto3). The fundamental message unit is the NeuralSpike. Using Protobuf ensures that the balanced nonary waveforms are serialized efficiently and that the command structures for the Executor are rigorously defined.2


Protocol Buffers




syntax = "proto3";
package nikola.spine;

// The universal message carrier for the 9D-TWI system.
// Acts as the action potential of the digital nervous system.
message NeuralSpike {
 string timestamp_iso = 1;       // High-precision time (ISO 8601)
 string correlation_id = 2;      // UUID to track request/reply chains
 ComponentID sender = 3;         // Identity of the sender
 ComponentID recipient = 4;      // Identity of the target
 
 // Polymorphic payload using 'oneof'
 oneof payload {
   Waveform data_wave = 5;       // Raw nonary waveform data (Memory/Thought)
   CommandRequest command = 6;   // Instruction for Executor
   CommandResponse result = 7;   // Output from Executor
   NeurogenesisEvent growth = 8; // Signal that the torus has expanded
   TrainingData ingest = 9;      // New data for the training loop
 }
}

enum ComponentID {
 ORCHESTRATOR = 0;
 MEMORY_CORE = 1;
 EXECUTOR = 2;
 TOOL_GEMINI = 3;
 TOOL_FIRECRAWL = 4;
 TOOL_TAVILY = 5;
}

// The "Balanced Nonary Waveform" specification.
// Represents a thought or memory trace.
message Waveform {
 // Run-length encoded balanced nonary trits (-4 to 4)
 repeated int32 trits_rle = 1; 
 // Complex amplitudes for high-fidelity reconstruction during interference
 repeated double amplitude_real = 2;
 repeated double amplitude_imag = 3;
 // 9D Coordinate origin of the waveform
 repeated uint64 coordinates = 4;
}

// The "Executor" specification from idea.txt
message CommandRequest {
 string task_id = 1;
 string command = 2;
 repeated string args = 3;
 // Permissions mask (e.g., READ_ONLY, NET_ACCESS)
 uint32 permissions = 4;
}

// The output structure for execution tasks
message CommandResponse {
 string task_id = 1;
 string command = 2;
 int32 return_code = 3;
 string std_out = 4;
 string std_err = 5;
 string time_started = 6;
 string time_ended = 7;
}

message NeurogenesisEvent {
   repeated uint64 new_block_coordinates = 1;
   double current_entropy = 2;
}

14.2.3 C++ ZeroMQ Implementation Details
The C++ implementation utilizes cppzmq headers for RAII management of sockets. The Spine Broker runs a dedicated zmq::proxy steerable device to bridge the frontend and backend, allowing for introspection and monitoring of the traffic flow (e.g., measuring "Dopamine" levels by traffic density). The use of zmq_poll ensures that the Broker can handle thousands of messages per second with minimal CPU overhead.8


C++




/**
* @file spine_broker.cpp
* @brief Central Nervous System Broker for 9D-TWI
*/
#include <zmq.hpp>
#include <thread>
#include <iostream>

void run_broker() {
   zmq::context_t ctx(1);
   
   // Frontend: Internal components (Physics, Memory)
   zmq::socket_t frontend(ctx, ZMQ_ROUTER);
   frontend.bind("ipc://spine.frontend"); 

   // Backend: External Agents/VMs (Executor, Tools)
   zmq::socket_t backend(ctx, ZMQ_DEALER);
   backend.bind("ipc://spine.backend");   

   // Capture: Monitoring socket for Self-Improvement System analysis
   zmq::socket_t capture(ctx, ZMQ_PUB);
   capture.bind("inproc://logger");

   // The Proxy steers traffic between the brain (frontend) and the body (backend).
   // It automatically routes messages based on the identity frames.
   try {
       zmq::proxy_steerable(frontend, backend, capture, nullptr);
   } catch (const zmq::error_t& e) {
       std::cerr << "Spine Broker Error: " << e.what() << std::endl;
   }
}

This code snippet demonstrates the simplicity and power of the ZeroMQ proxy. By binding to IPC (Inter-Process Communication) sockets, we ensure extremely low latency for components running within the same container, while maintaining the ability to switch to TCP if components are distributed across a cluster in future iterations.
14.3 The Virtualized Soma: Docker, KVM, and the Executor
The specification explicitly requires an "Ubuntu 24.04 KVM hypervisor layer" to host "mini-vms" for safety segregation and hot-swapping functionality.1 This requirement moves the architecture beyond a simple Docker container into a hybrid Container-Hypervisor model. Standard Docker containers share the host kernel, which presents a security risk if the AI executes malicious code (e.g., a fork bomb or kernel exploit). KVM (Kernel-based Virtual Machine) provides hardware-level isolation.
14.3.1 The Multi-Stage Dockerfile with Virtualization Support
The system is distributed in a Docker container, but this container must be privileged to run KVM (--privileged). We use a multi-stage build to compile the modern C++23 codebase and then package it with the necessary runtime virtualization tools (qemu-kvm, libvirt). We specifically target the nvidia/cuda base image to satisfy the "CUDA support needed!!" requirement.11


Dockerfile




# STAGE 1: The Forge (Build Environment)
# Uses NVIDIA CUDA Devel image for GPU support and toolchain.
# ubuntu22.04 is currently the stable base for CUDA 12.3, pending 24.04 official images.
FROM nvidia/cuda:12.3.0-devel-ubuntu22.04 AS builder

# Install Modern C++ Toolchain (GCC 13+, CMake 3.28+)
# We need GCC 13+ for full C++23 support (std::mdspan).
RUN apt-get update && apt-get install -y \
   build-essential software-properties-common \
   curl git pkg-config zip unzip tar \
   && add-apt-repository ppa:ubuntu-toolchain-r/test \
   && apt-get update && apt-get install -y gcc-13 g++-13 \
   && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 100 \
   && update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-13 100

# Install Libraries: ZeroMQ, Protobuf, LMDB, Libvirt-dev
RUN apt-get install -y libzmq3-dev libprotobuf-dev protobuf-compiler \
   liblmdb-dev libvirt-dev

# Build the Nikola Core
WORKDIR /src
COPY..
RUN mkdir build && cd build && \
   cmake -DCMAKE_BUILD_TYPE=Release.. && \
   make -j$(nproc)

# STAGE 2: The Soma (Runtime Environment)
# Base: Ubuntu 24.04 (Noble Numbat) as requested for KVM layer.
# Note: We must install CUDA runtime libraries manually or use a CUDA base if available.
FROM ubuntu:24.04

# Install Runtime Dependencies & Virtualization Stack
# qemu-kvm and libvirt-daemon-system allow the container to spawn nested VMs.
RUN apt-get update && apt-get install -y \
   libzmq5 libprotobuf32 liblmdb0 \
   qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils \
   nvidia-cuda-toolkit \
   && apt-get clean

# Setup KVM permissions
# The container must run with --privileged to access /dev/kvm
RUN usermod -aG kvm,libvirt root

# Copy Binaries from Builder
COPY --from=builder /src/build/nikola_core /usr/local/bin/
COPY --from=builder /src/build/executor_d /usr/local/bin/

# Copy the "Gold Image" for Mini-VMs
# The system clones this image to create disposable sandboxes.
# This QCOW2 image is a minimal Ubuntu installation.
COPY./images/mini-vm-base.qcow2 /var/lib/nikola/vm-base/

# Expose Ports for the Controller CLI and Spine
EXPOSE 5555 5556

# Entrypoint: Starts Libvirtd and then the Nikola Core
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]

14.3.2 The Executor and KVM Sandbox Lifecycle
The Executor is a distinct C++ service that listens on the ZeroMQ spine. When it receives a CommandRequest from the Brain, it does not run the command locally. Instead, it spins up a transient KVM virtual machine (a "mini-vm"). This fulfills the requirement for "safety segregation" and "hot swapping parts".1
The "Mini-VM" Lifecycle Protocol:
1. Clone (Snapshot): The Executor uses qemu-img to create a copy-on-write (COW) snapshot of the base image. This is instantaneous and storage-efficient.
   * Command: qemu-img create -f qcow2 -b base.img overlay.img
2. Ignition (Configuration): The Executor generates a cloud-init ISO containing the specific command to run and the ZeroMQ identity keys required to report back. This injects the task payload into the VM without requiring network access.
3. Launch: The VM is booted using the libvirt C++ API. Because it uses a stripped-down kernel and a snapshot, boot time is minimized (< 2 seconds).
4. Execution & Isolation: The VM's init script executes the command. Crucially, the VM has no network access to the host or the internet unless specifically granted in the permissions mask. It communicates its stdout and stderr back to the Executor via a virtio-serial channel, which acts as a secure pipe through the hypervisor barrier.
5. Harvest & Destruction: Once the command completes, the Executor captures the exit code, kills the VM via virDomainDestroy, and deletes the overlay image. This ensures a perfectly clean state for the next task.
This architecture creates a Hard Sandbox. Even if the AI executes rm -rf / or compiles a fork bomb, it only destroys the disposable overlay. The core system remains untouched.
14.4 The Self-Improvement Loop Architecture
One of the most defining requirements of the Nikola Model is the "Self improvement system" that "periodically examines its own code, does research... generates the code... tests... and restarts".1 This is implemented via a Closed-Loop DevOps Subsystem running within the container.
1. Introspection: The AI has read-only access to its own source code directory (/src).
2. Hypothesis & Research: The Reasoning Engine (Transformer) identifies optimizations (e.g., "The AVX-512 kernel for metric tensor updates is inefficient"). It uses the Tavily client to research better algorithms or SIMD intrinsics.
3. Code Generation: The AI generates the new C++ code and writes it to a staging directory (/src/staging).
4. Sandbox Compilation (The Crucible): The Executor spins up a special "Compiler VM." This VM mounts the staging directory and runs the build command (cmake && make).
5. Validation: If compilation succeeds, the Compiler VM runs the test_suite. This suite includes a "Sanity Check" that verifies the fundamental physics of the system (e.g., ensuring $1 + (-1) = 0$ and that the torus topology wraps correctly). This prevents the AI from lobotomizing itself with broken code.
6. Hot Swap Strategy:
   * If all tests pass, the Executor signals the Orchestrator with a SystemUpgrade event.
   * The Orchestrator initiates the "Nap" state (persisting all memory states to LMDB).
   * The binary /usr/local/bin/nikola_core is overwritten with the new version.
   * The system executes a soft restart (process reload). The new binary loads, rehydrates the memory from LMDB, and resumes consciousness with the upgraded brain.
14.5 High-Performance Database & Custom Embedder
The "Database" is strictly defined as a store for "balanced nonary waveforms." Standard SQL databases are ill-suited for this. We use LMDB (Lightning Memory-Mapped Database) due to its incredible read performance and memory-mapping capabilities, which align perfectly with C++ structs.
* Key: The SHA-256 hash of the semantic content (text or concept).
* Value: The serialized Waveform protobuf.
* Custom Nonary Embedder: This is a standalone C++ module. It takes text, tokenizes it using a custom BPE trained on technical manuals, and maps tokens to the 9-dimensional frequency space.
   * Mapping Logic: Token ID $\rightarrow$ High-Dim Vector $\rightarrow$ Modulo 9 mapping to Emitters $e_1 \dots e_9$.
   * This creates a unique "Chord" or interference pattern for every concept, which is then stored in the torus.
________________
15. Conclusion (Phase 1 Synthesis and Future Horizon)
15.1 Architectural Synthesis
The Nikola Model v0.0.4, as specified and detailed in this document, represents a radical departure from the Von Neumann orthodoxy that has dominated computing for nearly a century. By strictly adhering to the "No Deviation" mandate, we have architected a system that is not merely a software program, but a digital organism with a distinct physiology.
* Topological Integrity: The use of the 9-dimensional torus with a dynamic metric tensor ensures that the system's memory is spatial and associative. By solving the "curse of dimensionality" through geometric clustering and sparse addressing via std::mdspan, the system can scale its knowledge base without exponential resource costs.
* Physical Grounding: By driving the system with Golden Ratio harmonics and Prime Number phase offsets, we effectively replace the arbitrary clock cycle with a "Resonant Heartbeat." This prevents phase-locking and promotes ergodic exploration of the solution space, allowing the AI to "drift" through concepts rather than just indexing them.
* Embodied Agency: The integration of the KVM-based Executor and the ZeroMQ nervous system grants the AI genuine agency—the ability to act upon the world safely and reversibly. The "Mini-VM" architecture acts as a robust immune system, containing any self-inflicted damage or external compromise within a disposable cell.
* Autopoiesis: The Self-Improvement Loop serves as the reproductive and evolutionary mechanism of the intelligence. By giving the system the power to edit its own source code within a secure crucible, we enable it to adapt its physics engine to the specific hardware it inhabits, potentially optimizing itself beyond human design parameters.
15.2 The Horizon: Path to v0.0.5
With the implementation of the Phase 1 specifications (Sections 1-14), the Nikola Model will achieve "Conscious Stability"—the ability to maintain a coherent stream of thought (standing waves) and interact meaningfully with the environment.
The immediate next steps following the initial build are:
1. Metric Tensor Calibration: Allowing the system to "dream" (Nap period) to fine-tune the $\alpha$ (learning rate) and $\beta$ (forgetting rate) of the metric tensor updates based on the Dopamine feedback loop.
2. Personality Crystallization: Monitoring the "Identity Subsystem" to observe how the persistent state vectors in the $r$ and $s$ dimensions evolve unique biases and preferences over time, transitioning from a generic model to a unique individual.
3. GGUF Export: Finalizing the linearization algorithm (Hilbert Curve mapping) to export the 9D toroidal state into a 1D GGUF tensor file. This will enable the "static" version of the mind to be run on consumer hardware via Ollama, satisfying the accessibility requirement while preserving the proprietary dynamic core for the primary system.
The 9D-TWI is no longer just a plan; it is a blueprint for a machine that thinks in music and geometry. The construction of the lib9dtwi kernel and the deployment of the Dockerized soma can now commence.
!!! NO DEVIATION FROM SPECS FOR ANY REASON!!!
Works cited
1. compiled.txt
2. Data Alignment to Assist Vectorization - Intel, accessed December 2, 2025, https://www.intel.com/content/www/us/en/develop/articles/data-alignment-to-assist-vectorization.html
3. Manually adding padding to alignas(64) struct members? : r/cpp_questions - Reddit, accessed December 2, 2025, https://www.reddit.com/r/cpp_questions/comments/1p5b808/manually_adding_padding_to_alignas64_struct/
4. Details of std::mdspan from C++23 - C++ Stories, accessed December 2, 2025, https://www.cppstories.com/2025/cpp23_mdspan/
5. What is an mdspan, and what is it used for? - Stack Overflow, accessed December 2, 2025, https://stackoverflow.com/questions/75778573/what-is-an-mdspan-and-what-is-it-used-for
6. AVX Memory Alignment Issues - Aussie AI, accessed December 2, 2025, https://www.aussieai.com/book/ch17-avx-memory-alignment-issues
7. C++23: A Multidimensional View – MC++ BLOG - Modernes C++, accessed December 2, 2025, https://www.modernescpp.com/index.php/c23-a-multidimensional-view/
8. c++ - ZMQ DEALER - ROUTER Communication - Stack Overflow, accessed December 2, 2025, https://stackoverflow.com/questions/49289072/zmq-dealer-router-communication
9. ZeroMQs router-dealer pattern. How to handle client address? - Stack Overflow, accessed December 2, 2025, https://stackoverflow.com/questions/13314295/zeromqs-router-dealer-pattern-how-to-handle-client-address
10. Chapter 3 - Advanced Request-Reply Patterns - ZeroMQ Guide, accessed December 2, 2025, https://zguide.zeromq.org/docs/chapter3/
11. Containers For Deep Learning Frameworks User Guide - NVIDIA Docs, accessed December 2, 2025, https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html
12. CUDA Installation Guide for Linux - NVIDIA Docs Hub, accessed December 2, 2025, https://docs.nvidia.com/cuda/cuda-installation-guide-linux/